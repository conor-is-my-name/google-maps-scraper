import logging
import asyncio
import urllib.parse
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def extract_place_details(page, link, extract_full_details=False):
    """
    Extrait les détails d'un lieu (à utiliser avec un worker)
    """
    try:
        # Extraire URL
        href = await link.get_attribute('href')
        
        # Nom depuis URL (fallback)
        name = "N/A"
        if href and '/maps/place/' in href:
            try:
                name_part = href.split('/maps/place/')[1].split('/data=')[0]
                name = urllib.parse.unquote_plus(name_part)
            except:
                pass
        
        place_data = {
            'name': name,
            'url': href,
        }
        
        # Cliquer pour ouvrir le panneau détails
        logger.info(f"Processing: {name}")
        await link.click()
        await asyncio.sleep(2)  # Attendre chargement panneau
        
        # RATING
        rating = "N/A"
        try:
            rating_selectors = [
                'div[role="main"] span[role="img"][aria-label*="star"]',
                'div[role="main"] span[aria-label*="Star"]',
                'button[jsaction*="pane.rating"] span[role="img"]'
            ]
            for sel in rating_selectors:
                rating_elem = await page.query_selector(sel)
                if rating_elem:
                    aria = await rating_elem.get_attribute('aria-label')
                    if aria and 'star' in aria.lower():
                        rating = aria.split()[0]
                        break
        except Exception as e:
            logger.debug(f"Rating extraction error: {e}")
        
        # REVIEWS COUNT
        reviews_count = "0"
        try:
            reviews_selectors = [
                'button[jsaction*="pane.rating"] span[aria-label*="review"]',
                'button[aria-label*="review"]'
            ]
            for sel in reviews_selectors:
                reviews_elem = await page.query_selector(sel)
                if reviews_elem:
                    aria = await reviews_elem.get_attribute('aria-label')
                    if aria and 'review' in aria.lower():
                        parts = aria.split()
                        if parts:
                            reviews_count = ''.join(filter(str.isdigit, parts[0]))
                        break
        except Exception as e:
            logger.debug(f"Reviews extraction error: {e}")
        
        # CATEGORY
        category = "N/A"
        try:
            category_selectors = [
                'button[jsaction*="pane.rating.category"]',
                'div[role="main"] button[class*="DkEaL"]'
            ]
            for sel in category_selectors:
                cat_elem = await page.query_selector(sel)
                if cat_elem:
                    cat_text = await cat_elem.inner_text()
                    if cat_text and cat_text.strip():
                        category = cat_text.strip()
                        break
        except Exception as e:
            logger.debug(f"Category extraction error: {e}")
        
        place_data.update({
            'rating': rating,
            'reviews_count': reviews_count,
            'category': category,
        })
        
        # Détails supplémentaires si demandé
        if extract_full_details:
            # Téléphone
            phone = "N/A"
            try:
                phone_selectors = [
                    'button[data-item-id*="phone"]',
                    'button[aria-label*="Phone"]'
                ]
                for sel in phone_selectors:
                    phone_elem = await page.query_selector(sel)
                    if phone_elem:
                        phone_aria = await phone_elem.get_attribute('aria-label')
                        if phone_aria:
                            phone = phone_aria.split(':')[-1].strip()
                            break
            except Exception as e:
                logger.debug(f"Phone extraction error: {e}")
            
            # Site web
            website = "N/A"
            try:
                website_elem = await page.query_selector('a[data-item-id="authority"]')
                if website_elem:
                    website = await website_elem.get_attribute('href')
            except Exception as e:
                logger.debug(f"Website extraction error: {e}")
            
            # Adresse
            address = "N/A"
            try:
                address_elem = await page.query_selector('button[data-item-id="address"]')
                if address_elem:
                    address_aria = await address_elem.get_attribute('aria-label')
                    if address_aria:
                        address = address_aria.split(':')[-1].strip()
            except Exception as e:
                logger.debug(f"Address extraction error: {e}")
            
            # Horaires
            hours = "N/A"
            try:
                hours_selectors = [
                    'button[data-item-id*="oh"]',
                    'button[aria-label*="Hide open hours"]',
                    'button[aria-label*="Show open hours"]',
                    'div[aria-label*="Show open hours"]',
                    'div[aria-label*="Hide open hours"]'
                ]
                for sel in hours_selectors:
                    hours_elem = await page.query_selector(sel)
                    if hours_elem:
                        hours_aria = await hours_elem.get_attribute('aria-label')
                        if hours_aria:
                            hours = hours_aria
                            break
                
                # Fallback: chercher texte Open/Closed
                if hours == "N/A":
                    status_texts = await page.query_selector_all('div[role="main"] span')
                    for span in status_texts:
                        text = await span.inner_text()
                        if text and ('open' in text.lower() or 'closed' in text.lower() or 'pm' in text.lower() or 'am' in text.lower()):
                            hours = text
                            break
                            
            except Exception as e:
                logger.debug(f"Hours extraction error: {e}")
            
            place_data.update({
                'phone': phone,
                'website': website,
                'address': address,
                'hours': hours,
            })
        
        logger.info(f"✓ Extracted: {name} (rating: {rating})")
        return place_data
        
    except Exception as e:
        logger.error(f"Error extracting place: {e}")
        return None


async def process_places_parallel(context, place_links, max_places, extract_details, max_workers=3):
    """
    Traite plusieurs lieux en parallèle avec plusieurs onglets
    
    Args:
        context: Browser context
        place_links: Liste des liens à traiter
        max_places: Nombre max de lieux
        extract_details: Mode détails
        max_workers: Nombre de workers parallèles (3-5 recommandé)
    """
    results = []
    semaphore = asyncio.Semaphore(max_workers)  # Limite le nombre de workers simultanés
    
    async def worker(link):
        async with semaphore:
            # Créer un nouvel onglet pour ce worker
            page = await context.new_page()
            try:
                result = await extract_place_details(page, link, extract_details)
                return result
            finally:
                await page.close()
    
    # Lancer tous les workers en parallèle
    tasks = [worker(link) for link in place_links[:max_places]]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Filtrer les résultats valides
    valid_results = [r for r in results if r is not None and not isinstance(r, Exception)]
    
    return valid_results


async def scrape_google_maps(query: str, max_places: int = 10, lang: str = "en", headless: bool = True, extract_details: bool = False):
    """
    Scrape Google Maps avec parallélisation
    
    Args:
        query: Requête de recherche
        max_places: Nombre maximum de résultats
        lang: Code langue
        headless: Mode headless
        extract_details: Extraire tous les détails (téléphone, horaires, etc.)
    """
    results = []
    
    async with async_playwright() as p:
        try:
            logger.info(f"Launching browser (headless={headless})...")
            browser = await p.chromium.launch(
                headless=headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=IsolateOrigins,site-per-process',
                    '--disable-web-security',
                    '--disable-site-isolation-trials',
                    '--disable-gpu',
                    '--disable-software-rasterizer',
                    '--disable-extensions',
                    '--single-process',
                ]
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale=lang,
                timezone_id='Europe/Paris',
                geolocation={'latitude': 48.8566, 'longitude': 2.3522},
                permissions=['geolocation']
            )
            
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
                Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en', 'fr'] });
                window.chrome = { runtime: {} };
            """)
            
            page = await context.new_page()
            
            search_url = f"https://www.google.com/maps/search/{query.replace(' ', '+')}?hl={lang}"
            logger.info(f"Navigating to: {search_url}")
            
            await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
            await asyncio.sleep(2)
            
            # Gérer consentement
            logger.info("Checking for consent form...")
            consent_selectors = [
                'button:has-text("Accept all")',
                'button:has-text("Tout accepter")',
            ]
            
            for selector in consent_selectors:
                try:
                    consent_button = await page.wait_for_selector(selector, timeout=3000)
                    if consent_button:
                        await consent_button.click()
                        logger.info(f"Clicked consent button")
                        await asyncio.sleep(2)
                        break
                except PlaywrightTimeout:
                    continue
            
            # Attendre résultats
            logger.info("Waiting for results...")
            feed_element = await page.wait_for_selector('div[role="feed"]', timeout=10000, state='visible')
            
            if not feed_element:
                raise Exception("Feed element not found.")
            
            # Scroll pour charger les lieux
            logger.info("Scrolling to load places...")
            places_loaded = 0
            scroll_attempts = 0
            
            while places_loaded < max_places and scroll_attempts < 20:
                current_places = await page.query_selector_all('a[href*="/maps/place/"]')
                places_loaded = len(current_places)
                logger.info(f"Loaded {places_loaded} places...")
                
                if places_loaded >= max_places:
                    break
                
                await feed_element.evaluate('element => element.scrollTop = element.scrollHeight')
                await asyncio.sleep(1.5)
                scroll_attempts += 1
            
            # Récupérer tous les liens
            place_links = await page.query_selector_all('a[href*="/maps/place/"]')
            logger.info(f"Starting parallel extraction of {min(len(place_links), max_places)} places...")
            
            # Traitement parallèle avec 3 workers
            results = await process_places_parallel(
                context, 
                place_links, 
                max_places, 
                extract_details,
                max_workers=3  # 3 onglets en parallèle
            )
            
            await page.close()
            await browser.close()
            
        except Exception as e:
            logger.error(f"An error occurred during scraping: {e}")
            if 'browser' in locals():
                await browser.close()
            raise
    
    logger.info(f"Scraping finished. Found {len(results)} places.")
    return results


async def main():
    """Test function"""
    import time
    
    print("\n=== TEST PARALLÈLE (3 workers) ===")
    start = time.time()
    results = await scrape_google_maps("restaurant sannois", max_places=6, lang="en", extract_details=True)
    elapsed = time.time() - start
    
    print(f"\n✓ Extracted {len(results)} places in {elapsed:.1f} seconds")
    print(f"  Average: {elapsed/len(results):.1f}s per place")
    
    for place in results[:3]:  # Afficher les 3 premiers
        print(f"\n{place['name']}")
        print(f"  Rating: {place.get('rating', 'N/A')}")
        print(f"  Category: {place.get('category', 'N/A')}")
        print(f"  Phone: {place.get('phone', 'N/A')}")


if __name__ == "__main__":
    asyncio.run(main())
